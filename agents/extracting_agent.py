# agents/03_extracting_agent.py
"""
03. ExtractingAgent - 마크다운 파싱 및 메타데이터 추출 에이전트

마크다운 파일을 파싱하고 필요한 메타데이터를 추출합니다.
schema/posts.schema.sql 스키마와 호환되는 데이터를 생성합니다.

역할:
- 마크다운 파싱 (frontmatter + content)
- 이미지 경로 추출 및 썸네일 감지
- slug 생성 (PostSchema 호환)
- 단어 수 계산
- 읽기 시간 계산
- LLM을 통한 태그 생성 (5-7개)
- LLM을 통한 3문장 요약 생성
- PostSchema 검증
"""

import re
import os
from typing import Dict, Any, List, Optional
from pathlib import Path
import frontmatter
from .base import BaseAgent
from schema import calculate_reading_time, generate_slug_from_title, PostSchema, PostStatusEnum


class ExtractingAgent(BaseAgent):
    """
    마크다운 파일 파싱 및 메타데이터 추출 에이전트

    작업:
    1. 마크다운 파일 읽기 및 frontmatter 파싱
    2. 본문에서 이미지 경로 추출
    3. 기본 메타데이터 생성 (slug, word count 등)
    4. LLM으로 tags와 description 생성 (항상 사용)
    """

    def __init__(self, llm=None, enable_llm: bool = True):
        """
        Initialize ExtractingAgent.

        Args:
            llm: Pre-configured LLM instance (optional). If None and enable_llm=True,
                 will auto-create from llm_factory based on config.LLM_PROVIDER
            enable_llm: Enable LLM for tags/description generation. Default: True
        """
        super().__init__(
            name="ExtractingAgent",
            description="Markdown parsing and metadata extraction",
        )
        self.enable_llm = enable_llm
        self.llm = llm  # Will be set in execute() if None and enable_llm=True

    async def execute(self, task: Dict[str, Any]) -> Dict[str, Any]:
        """
        작업 실행

        Expected task data:
            - file_path: 마크다운 파일 경로 (또는)
            - article_info: DocumentScanner의 결과
            - extract_metadata: bool (LLM으로 태그/요약 생성 여부, deprecated - now always True if enable_llm)
            - db_schema: PostSchema (필드 검증용)

        Note:
            Tags and description are NOW ALWAYS GENERATED by LLM (not from frontmatter),
            unless enable_llm=False in __init__.
        """
        action = task.get("action")

        # Initialize LLM if needed (lazy initialization)
        if self.enable_llm and self.llm is None:
            try:
                from llm_factory import create_llm
                self.llm = create_llm()
                self._log(f"LLM initialized: {self.llm.__class__.__name__}")
            except Exception as e:
                self._log(f"Failed to initialize LLM: {e}. Continuing without LLM.", "warning")
                self.enable_llm = False

        # DocumentScanner 결과 사용 또는 직접 파일 경로
        article_info = task["data"].get("article_info")
        if article_info:
            file_path = article_info.get("md_file")
            # Extract full category hierarchy from folder path
            categories = article_info.get("categories", [])  # NEW: list of categories
            category = article_info.get("category")          # Backward compat
            subcategory = article_info.get("subcategory")    # Backward compat
            thumbnail = article_info.get("thumbnail")
            predefined_images = article_info.get("images", [])
        else:
            file_path = task["data"].get("file_path")
            # Extract categories from folder path structure
            categories = self._extract_categories_from_path(file_path)
            category = categories[0] if len(categories) > 0 else None
            subcategory = categories[1] if len(categories) > 1 else None
            # Auto-detect thumbnail and images from article folder
            thumbnail, predefined_images = self._detect_article_assets(file_path)

        if not file_path:
            return {"success": False, "error": "No file_path or article_info provided"}

        if not os.path.exists(file_path):
            return {"success": False, "error": f"File not found: {file_path}"}

        try:
            # 1. 파일 읽기 및 파싱
            self._log(f"Parsing file: {file_path}")
            parsed_data = self._parse_markdown(file_path)

            # 2. 카테고리 정보 추가 (DocumentScanner에서 온 경우)
            if categories:
                parsed_data["categories"] = categories  # Full hierarchy
                category_path = '/'.join(categories)
                self._log(f"Categories: {category_path}")
            # Backward compatibility
            if category:
                parsed_data["category"] = category
                parsed_data["subcategory"] = subcategory

            # 3. 이미지 처리
            if predefined_images:
                # DocumentScanner에서 발견한 이미지 사용
                parsed_data["images"] = [
                    {"alt": "", "local_path": img, "s3_url": None} for img in predefined_images
                ]
                if thumbnail:
                    parsed_data["thumbnail"] = {
                        "alt": f"{parsed_data['title']} thumbnail",
                        "local_path": thumbnail,
                        "s3_url": None,
                    }
            else:
                # 마크다운 본문에서 이미지 추출
                self._log(f"Extracting images from content...")
                base_dir = os.path.dirname(file_path)
                images, detected_thumbnail = self._extract_images(parsed_data["content"], base_dir)
                parsed_data["images"] = images
                if detected_thumbnail:
                    parsed_data["thumbnail"] = detected_thumbnail

            self._log(f"Found {len(parsed_data.get('images', []))} image(s)")
            if parsed_data.get("thumbnail"):
                self._log(f"Detected thumbnail: {parsed_data['thumbnail']['local_path']}")

            # 4. 기본 메타데이터 생성
            # Prefer article_name (filename-based) for slug, fall back to title
            article_name = article_info.get("article_name") if article_info else None
            parsed_data["slug"] = generate_slug_from_title(article_name or parsed_data["title"])
            parsed_data["word_count"] = len(parsed_data["content"].split())

            # 읽기 시간 계산 (250 wpm 기준)
            parsed_data["reading_time"] = calculate_reading_time(parsed_data["word_count"])

            self._log(
                f"Word count: {parsed_data['word_count']}, Reading time: {parsed_data['reading_time']} min"
            )

            # 5. LLM으로 태그/description 생성 (기본 동작)
            # Use existing tags/desc from frontmatter as hints for LLM
            existing_tags = parsed_data.get("tags", [])
            existing_desc = parsed_data.get("description", "")

            if self.enable_llm and self.llm:
                self._log("Generating tags and description with LLM...")
                metadata = await self._generate_metadata_with_llm(
                    parsed_data["title"],
                    parsed_data["content"],
                    categories,  # Pass full category hierarchy
                    existing_tags=existing_tags,  # Pass existing tags as hints
                    existing_desc=existing_desc,  # Pass existing desc as hint
                )
                # Override frontmatter tags and description with LLM-generated ones
                parsed_data["tags"] = metadata.get("tags", [])
                parsed_data["description"] = metadata.get("summary", "")
                parsed_data["summary"] = metadata.get("summary", "")  # Alias for description
                self._log(f"✓ Generated {len(parsed_data['tags'])} tags and description")
            else:
                # LLM disabled: fallback to frontmatter or empty
                self._log("LLM disabled: using frontmatter tags/description or empty", "warning")
                if not parsed_data.get("tags"):
                    parsed_data["tags"] = []
                if not parsed_data.get("description"):
                    parsed_data["description"] = ""
                parsed_data["summary"] = parsed_data.get("description", "")

            return {"success": True, "data": parsed_data, "agent": self.name}

        except Exception as e:
            return {"success": False, "error": str(e), "agent": self.name}

    def _parse_markdown(self, file_path: str) -> Dict[str, Any]:
        """
        마크다운 파일 파싱 (frontmatter 기반)

        Extracts metadata from YAML frontmatter and returns data compatible with PostSchema.
        Supports fields: title, userId, tags, desc, date, author, status
        """
        with open(file_path, "r", encoding="utf-8") as f:
            post = frontmatter.load(f)

        # frontmatter에서 메타데이터 추출
        metadata = post.metadata

        # Extract title (required for PostSchema)
        title = metadata.get("title", self._extract_title_from_content(post.content))

        # Map frontmatter fields to PostSchema-compatible fields
        return {
            "file_path": file_path,
            "title": title,
            "user_id": metadata.get("userId", 1),  # Maps userId -> user_id (default: 1)
            "username": metadata.get("username", ""),  # Username for URL
            "description": metadata.get("desc", ""),  # Maps desc -> description
            "status": self._map_status(metadata.get("status")),  # Map to PostStatusEnum
            "author": metadata.get("author") or metadata.get("username", "Unknown"),
            "date": metadata.get("date", ""),
            "tags": metadata.get("tags", []),  # Already a list
            "category": metadata.get("category", ""),
            "content": post.content,
            "raw_frontmatter": metadata,
        }

    def _map_status(self, status_value: Any) -> str:
        """
        Map frontmatter status to PostStatusEnum values.

        SQL Schema: CREATE TYPE post_status_enum AS ENUM ('public', 'private', 'follower');

        Supported frontmatter formats:
        1. String (recommended): status: 'public', status: 'private', status: 'follower'
        2. Boolean (backward compat): status: true (-> 'public'), status: false (-> 'private')
        3. Missing/None: defaults to 'public'

        Args:
            status_value: Value from frontmatter (str, bool, or None)

        Returns:
            One of: 'public', 'private', 'follower' (matches PostStatusEnum)

        Examples:
            >>> _map_status('public')    # 'public'
            >>> _map_status('private')   # 'private'
            >>> _map_status('follower')  # 'follower'
            >>> _map_status(True)        # 'public'
            >>> _map_status(False)       # 'private'
            >>> _map_status(None)        # 'public' (default)
            >>> _map_status('invalid')   # 'public' (fallback)
        """
        if status_value is None:
            return PostStatusEnum.PUBLIC

        # Boolean (backward compatibility): true = public, false = private
        if isinstance(status_value, bool):
            return PostStatusEnum.PUBLIC if status_value else PostStatusEnum.PRIVATE

        # String: validate against enum values
        status_str = str(status_value).lower().strip()
        if status_str in ["public", "private", "follower"]:
            return status_str

        # Invalid value: log warning and default to public
        self._log(f"Invalid status value '{status_value}', defaulting to 'public'", "warning")
        return PostStatusEnum.PUBLIC

    def _extract_title_from_content(self, content: str) -> str:
        """본문에서 제목 추출 (frontmatter에 없을 경우)"""
        # 첫 번째 # 헤더 찾기
        match = re.search(r"^#\s+(.+)$", content, re.MULTILINE)
        if match:
            return match.group(1).strip()
        return "Untitled"

    def _extract_categories_from_path(self, file_path: str) -> List[str]:
        """
        Extract category hierarchy from folder path structure.

        Folder structure convention:
            posts/category1/category2/.../categoryN/[article-slug]/[article-slug].md

        Example:
            posts/technology/ai/langchain-guide/langchain-guide.md
            -> categories: ['technology', 'ai']

        Args:
            file_path: Full path to the markdown file

        Returns:
            List of category names (excluding the article folder)
        """
        from config import POSTS_DIR
        from pathlib import Path

        if not file_path:
            return []

        try:
            path = Path(file_path)
            article_folder = path.parent

            # Try to get path relative to POSTS_DIR
            try:
                relative_path = article_folder.relative_to(POSTS_DIR)
                path_parts = list(relative_path.parts)
            except ValueError:
                # File is not under POSTS_DIR, try PROJECT_ROOT/docs
                from config import PROJECT_ROOT
                docs_dir = PROJECT_ROOT / "docs"
                try:
                    relative_path = article_folder.relative_to(docs_dir)
                    path_parts = list(relative_path.parts)
                except ValueError:
                    # Cannot determine relative path
                    return []

            # path_parts example: ['technology', 'ai', 'langchain-guide']
            # The last part is the article folder (same name as .md file)
            # Categories are everything before that
            if len(path_parts) > 1:
                categories = path_parts[:-1]  # Exclude article folder
            else:
                categories = []

            return categories

        except Exception as e:
            self._log(f"Failed to extract categories from path: {e}", "warning")
            return []

    def _detect_article_assets(self, file_path: str) -> tuple:
        """
        Auto-detect thumbnail and content images from article folder.

        Convention:
            - Thumbnail: image with same name as article (e.g., langchain-guide.png)
            - Content images: all other images in the folder

        Args:
            file_path: Full path to the markdown file

        Returns:
            Tuple of (thumbnail_path, list_of_image_paths)
        """
        from pathlib import Path

        if not file_path:
            return None, []

        try:
            path = Path(file_path)
            article_folder = path.parent
            article_name = path.stem

            # Skip if folder doesn't match article name convention
            if article_folder.name != article_name:
                return None, []

            image_extensions = {".png", ".jpg", ".jpeg", ".gif", ".webp", ".svg"}
            thumbnail = None
            images = []

            for item in article_folder.iterdir():
                if not item.is_file():
                    continue
                if item.suffix.lower() not in image_extensions:
                    continue

                # Check if this is the thumbnail (same name as article)
                if item.stem == article_name:
                    thumbnail = str(item)
                else:
                    images.append(str(item))

            return thumbnail, sorted(images)

        except Exception as e:
            self._log(f"Failed to detect article assets: {e}", "warning")
            return None, []

    def _extract_images(self, content: str, base_dir: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        본문에서 이미지 경로 추출 및 썸네일 감지

        Detects images in markdown with ![alt](path) syntax.
        Identifies thumbnails when alt text contains "thumbnail".

        Args:
            content: Markdown content
            base_dir: Base directory for resolving relative paths

        Returns:
            List of image dicts with alt, local_path, s3_url, is_thumbnail
        """
        # ![alt](path) 형식의 이미지 찾기
        pattern = r"!\[([^\]]*)\]\(([^\)]+)\)"
        matches = re.findall(pattern, content)

        images = []
        thumbnail = None

        for alt, path in matches:
            # Resolve relative paths if base_dir provided
            if base_dir and not os.path.isabs(path):
                resolved_path = os.path.join(base_dir, path)
            else:
                resolved_path = path

            # Detect thumbnail from alt text
            is_thumbnail = "thumbnail" in alt.lower()

            image_data = {
                "alt": alt,
                "local_path": resolved_path,
                "s3_url": None,  # UploadingAgent will fill this
                "is_thumbnail": is_thumbnail,
            }

            if is_thumbnail and thumbnail is None:
                # First thumbnail found
                thumbnail = image_data
            else:
                # Regular image
                images.append(image_data)

        return images, thumbnail


    async def _generate_metadata_with_llm(
        self,
        title: str,
        content: str,
        categories: List[str] = None,
        existing_tags: List[str] = None,
        existing_desc: str = None,
    ) -> Dict[str, Any]:
        """
        LLM을 사용하여 메타데이터 생성

        Args:
            title: Article title
            content: Article content (full markdown)
            categories: Full category hierarchy from folder path
                       Example: ['technology', 'ai', 'langchain']
                       or ['java', 'spring', 'jdbc']
            existing_tags: Existing tags from frontmatter (used as hints)
            existing_desc: Existing description from frontmatter (used as hint)

        Returns:
            Dictionary with:
            - tags: 5-7 relevant keywords
            - summary: Exactly 3 sentences (for card display)

        Note:
            If existing_tags or existing_desc are provided, they are used as hints
            to guide the LLM generation while still allowing refinement.
        """
        if not self.llm:
            return {"tags": [], "summary": "No summary available."}

        # 본문 일부만 사용 (토큰 절약)
        content_preview = content[:1500]

        # 카테고리 정보 추가 - Full hierarchy
        category_info = ""
        if categories and len(categories) > 0:
            category_path = ' > '.join(categories)
            category_info = f"\nCategory Path: {category_path}"

        # Build hints section from existing frontmatter
        hints_section = ""
        if existing_tags or existing_desc:
            hints_section = "\n\nEXISTING HINTS (use as reference, refine if needed):"
            if existing_tags:
                hints_section += f"\nExisting tags: {', '.join(existing_tags)}"
            if existing_desc:
                hints_section += f"\nExisting description: {existing_desc}"

        prompt = f"""You are analyzing a blog article to extract metadata for a card display and search indexing.

ARTICLE INFO:
Title: {title}{category_info}

Content preview:
{content_preview}{hints_section}

EXTRACT THE FOLLOWING:

1. **Tags** (5-7 keywords):
   - Choose relevant keywords for search and categorization
   - Include technical terms, topics, and concepts
   - If existing tags are provided, use them as reference and refine/expand
   - Comma-separated list

2. **Summary** (EXACTLY 3 sentences):
   - First sentence: What is this article about?
   - Second sentence: What will readers learn?
   - Third sentence: Key takeaway or benefit
   - Keep each sentence concise (max 100 characters)
   - If existing description is provided, use it as a base and refine
   - This will be displayed on article cards

Respond in this EXACT format:
TAGS: tag1, tag2, tag3, tag4, tag5
SUMMARY: First sentence here. Second sentence here. Third sentence here.
"""

        try:
            # LLM 호출
            response = self.llm.invoke(prompt)
            result_text = response.content

            # 파싱
            tags_match = re.search(r"TAGS:\s*(.+)", result_text, re.IGNORECASE)
            summary_match = re.search(r"SUMMARY:\s*(.+)", result_text, re.IGNORECASE | re.DOTALL)

            # Tags 추출
            tags = []
            if tags_match:
                tags_str = tags_match.group(1).strip()
                tags = [tag.strip() for tag in tags_str.split(",") if tag.strip()]
                # 5-7개로 제한
                tags = tags[:7] if len(tags) > 7 else tags

            # Summary 추출
            summary = "No summary available."
            if summary_match:
                summary_text = summary_match.group(1).strip()
                # 여러 줄일 수 있으므로 정리
                summary = " ".join(summary_text.split())

                # 정확히 3문장인지 확인 (간단한 체크)
                sentences = [s.strip() for s in summary.split(".") if s.strip()]
                if len(sentences) >= 3:
                    summary = ". ".join(sentences[:3]) + "."

            self._log(f"Generated {len(tags)} tags", "success")
            self._log(f"Generated summary: {len(summary)} chars", "success")

            return {"tags": tags, "summary": summary}

        except Exception as e:
            self._log(f"LLM metadata generation failed: {e}", "warning")
            return {"tags": [], "summary": "No summary available."}

    async def _generate_og_alt(self, title: str, content: str = "") -> str:
        """
        Generate SEO-friendly alt text for OG image using LLM.

        Args:
            title: Article title
            content: Article content (optional, for context)

        Returns:
            Alt text string (max 125 chars)
        """
        if not self.llm:
            return f"{title} thumbnail"

        prompt = f"""Generate a brief, descriptive alt text for a blog post thumbnail image.
The alt text should be accessible and SEO-friendly.

Article Title: {title}

Requirements:
- Maximum 100 characters
- Describe what a reader would expect to see
- Include relevant keywords naturally
- Do not start with "Image of" or "Picture of"

Alt text:"""

        try:
            response = self.llm.invoke(prompt)
            alt_text = response.content.strip()
            
            # Clean and truncate
            alt_text = alt_text.strip('"\'')
            if len(alt_text) > 125:
                alt_text = alt_text[:122] + "..."
            
            self._log(f"Generated OG alt text: {alt_text[:50]}...")
            return alt_text
            
        except Exception as e:
            self._log(f"OG alt generation failed: {e}", "warning")
            return f"{title} thumbnail"
